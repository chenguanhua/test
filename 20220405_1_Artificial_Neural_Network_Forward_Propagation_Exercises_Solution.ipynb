{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a neural network in python\n",
    "\n",
    "Firstly, a neural network is defined by the number of layers, and the number of neurons in each layer.\n",
    "\n",
    "Let us use a list to denote this.\n",
    "\n",
    "## Defining layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the sizes of the layers in our neural network\n",
    "layers = [2, 2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code denotes the 3-neuron neural network we saw previously: 2-dimensional input, 2 neurons in a hidden layer, 1 neuron in the output layer.\n",
    "\n",
    "Generally speaking, a neural network than has more than 1 hidden layer is a **deep** neural network.\n",
    "\n",
    "## Defining weight matrices\n",
    "\n",
    "Using the sizes of the layers in our neural network, let us initialize the weight matrices to random values (sampled from a standard normal gaussian, because we know that we need both positive and negative weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights=[np.random.randn(i+1,o) for i,o in zip(layers[:-1],layers[1:])]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3, 2)\n",
      "[[-0.43778315  2.171257  ]\n",
      " [ 1.15231025 -1.81881234]\n",
      " [-0.13804934  0.53983961]]\n",
      "2\n",
      "(3, 1)\n",
      "[[-1.77528229]\n",
      " [ 1.31487654]\n",
      " [-0.47344805]]\n"
     ]
    }
   ],
   "source": [
    "# Displaying weight matrices\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    print(i+1)\n",
    "    print(weights[i].shape)\n",
    "    print(weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "The output of the neural network is calculated by **propagating forward** the outputs of each layer.\n",
    "\n",
    "Let us define our input as an np.array, since we want to represent matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We shall use np.array() to represent matrices\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding bias terms\n",
    "\n",
    "Since the input to every layer needs a bias term (1) added to it, let us define a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "    X=np.array(X)\n",
    "    X=np.c_[np.ones(X.shape[0]),X]\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell to test the addBiasTerms function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding bias terms: \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "After adding bias terms: \n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# TESTING addBiasTerms\n",
    "\n",
    "# We shall use np.array() to represent matrices\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "print(\"Before adding bias terms: \"); print(X)\n",
    "X = addBiasTerms(X)\n",
    "print(\"After adding bias terms: \"); print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "Let us also define a function to calculate the sigmoid of any np.array given to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation of inputs\n",
    "\n",
    "Let us store the outputs of the layers in a list called \"outputs\". We shall use that the output of one layer as the input to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        print(inputs,w)\n",
    "        outputs.append(sigmoid(np.dot(inputs, w)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(3, 3)\n",
      "[[ 0.27626589 -1.85462808  0.62390111]\n",
      " [ 1.14531129  1.03719047  1.88663893]\n",
      " [-0.11169829 -0.36210134  0.14867505]]\n",
      "2\n",
      "(4, 3)\n",
      "[[-0.43778315  2.171257    1.15231025]\n",
      " [-1.81881234 -0.13804934  0.53983961]\n",
      " [-1.77528229  1.31487654 -0.47344805]\n",
      " [-1.0922299  -0.25002744 -0.9822943 ]]\n",
      "3\n",
      "(4, 1)\n",
      "[[ 1.03126909]\n",
      " [ 0.49133378]\n",
      " [-0.4466466 ]\n",
      " [-0.80636008]]\n",
      "X:\n",
      "[[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]] [[ 0.27626589 -1.85462808  0.62390111]\n",
      " [ 1.14531129  1.03719047  1.88663893]\n",
      " [-0.11169829 -0.36210134  0.14867505]]\n",
      "[[1.         0.56863052 0.13533042 0.65110527]\n",
      " [1.         0.5410493  0.09825821 0.6840779 ]\n",
      " [1.         0.80558555 0.30630785 0.92487742]\n",
      " [1.         0.78749289 0.2351351  0.93457669]] [[-0.43778315  2.171257    1.15231025]\n",
      " [-1.81881234 -0.13804934  0.53983961]\n",
      " [-1.77528229  1.31487654 -0.47344805]\n",
      " [-1.0922299  -0.25002744 -0.9822943 ]]\n",
      "[[1.         0.08140402 0.89166928 0.68040434]\n",
      " [1.         0.08758728 0.88642415 0.67390581]\n",
      " [1.         0.03056216 0.90304791 0.63033974]\n",
      " [1.         0.03528793 0.89454661 0.63368921]] [[ 1.03126909]\n",
      " [ 0.49133378]\n",
      " [-0.4466466 ]\n",
      " [-0.80636008]]\n",
      "outputs:\n",
      "1\n",
      "(4, 3)\n",
      "[[0.56863052 0.13533042 0.65110527]\n",
      " [0.5410493  0.09825821 0.6840779 ]\n",
      " [0.80558555 0.30630785 0.92487742]\n",
      " [0.78749289 0.2351351  0.93457669]]\n",
      "2\n",
      "(4, 3)\n",
      "[[0.08140402 0.89166928 0.68040434]\n",
      " [0.08758728 0.88642415 0.67390581]\n",
      " [0.03056216 0.90304791 0.63033974]\n",
      " [0.03528793 0.89454661 0.63368921]]\n",
      "3\n",
      "(4, 1)\n",
      "[[0.53104842]\n",
      " [0.53369251]\n",
      " [0.53361453]\n",
      " [0.5344651 ]]\n"
     ]
    }
   ],
   "source": [
    "# VIEWING FORWARD PROPAGATION\n",
    "# Set random seeds\n",
    "np.random.seed(17)\n",
    "\n",
    "# Initialize network\n",
    "layers = [2, 3, 3, 1]\n",
    "#weights = initializeWeights(layers)\n",
    "\n",
    "# 3-neuron network\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Input\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "print(\"X:\"); print(X)\n",
    "\n",
    "# Forward propagate X, and save outputs\n",
    "outputs = forwardProp(X, weights)\n",
    "\n",
    "print(\"outputs:\")\n",
    "for o in range(len(outputs)):\n",
    "    print(o+1); print(outputs[o].shape); print(outputs[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
